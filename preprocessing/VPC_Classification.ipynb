{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verb-Particle Constructions\n",
    "### Distinguishing VPCs from verb-preposition combinations\n",
    "\n",
    "We use the dataset in [Tu and Roth (2012)](http://www.aclweb.org/anthology/S12-1010) which constains 1,348 sentences from BNC. 65% of the sentences contain a VPC and 35% contain a verb-preposition combination. The dataset is focused on 23 different phrasal verbs that they define as the most confusing. They are derived from six of the most frequently used verbs: _take, make, have, get, do_ and _give_, and their combination with common prepositions or particles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(133)\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import json\n",
    "import codecs\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "from statsmodels.stats.inter_rater import fleiss_kappa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, download the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('vpc/pvcData.txt'):\n",
    "    !mkdir -p vpc\n",
    "    !wget http://cogcomp.org/software/tools/pvcDataSubmission.tar.gz\n",
    "    !tar -zxvf pvcDataSubmission.tar.gz\n",
    "    !mv dataSubmission/pvcData.txt vpc\n",
    "    !rm -r dataSubmission\n",
    "    !rm -r pvcDataSubmission.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with codecs.open('vpc/pvcData.txt', 'r', 'utf-8') as f_in:\n",
    "    dataset = [line.strip().split('\\t') for line in f_in]\n",
    "    \n",
    "# The dataset fields are: bnc_id, annotation confidence, label (true/false), \n",
    "# stem of the phrasal verb, pvcIndex\n",
    "# We convert it to: bnc_id, label, and stem. We can disregard pvc_index because we re-tokenize.\n",
    "dataset = [(bnc_id, label, stem) for bnc_id, _, label, stem, pvc_index in dataset]\n",
    "\n",
    "print('Dataset size: {}'.format(len(dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset to train/validation/test. The split is lexical by verb, to make it more difficult."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_lexically(dataset, word_index=0):\n",
    "    \"\"\"\n",
    "    Split the dataset to train, test, and validation, such that\n",
    "    the word in word_index (0 = verb, 1 = preposition) doesn't\n",
    "    repeat across sets.\n",
    "    \"\"\"\n",
    "    instances_per_w = defaultdict(list)\n",
    "    [instances_per_w[span_text.split('_')[word_index]].append(\n",
    "        (bnc_id, label, span_text)) \n",
    "     for (bnc_id, label, span_text) in dataset]\n",
    "\n",
    "    train, test, val = [], [], []\n",
    "    train_size = 8 * len(dataset) // 10\n",
    "    val_size = test_size = len(dataset) // 10\n",
    "\n",
    "    words = [w for w, examples in sorted(instances_per_w.items(), key=lambda x: len(x[1]))]\n",
    "    w_index = 0\n",
    "\n",
    "    while len(test) < test_size:\n",
    "        test += instances_per_w[words[w_index]]\n",
    "        w_index += 1\n",
    "\n",
    "    print('Test set size: {} (needed: {})'.format(len(test), test_size))\n",
    "\n",
    "    while len(val) < val_size:\n",
    "        val += instances_per_w[words[w_index]]\n",
    "        w_index += 1\n",
    "\n",
    "    print('Validation set size: {} (needed: {})'.format(len(val), val_size))\n",
    "\n",
    "    train = [example for i in range(w_index, len(words)) \n",
    "             for example in instances_per_w[words[i]]]\n",
    "    print('Train set size: {} (needed: {})'.format(len(train), train_size))\n",
    "\n",
    "    # Check the label distribution in the test set\n",
    "    ctr = Counter([label for (bnc_id, label, span_text) in test])\n",
    "    assert(ctr['false'] / ctr['true'] <= 4 and ctr['true'] / ctr['false'] <= 4)\n",
    "    \n",
    "    # Make sure the split is lexical among verbs\n",
    "    test_words = [span_text.split('_')[word_index] for _, _, span_text in test]\n",
    "    train_words = [span_text.split('_')[word_index] for _, _, span_text in train]\n",
    "    val_words = [span_text.split('_')[word_index] for _, _, span_text in val]\n",
    "    assert(len(set(train_words).intersection(set(val_words))) == 0)\n",
    "    assert(len(set(train_words).intersection(set(test_words))) == 0)\n",
    "    assert(len(set(test_words).intersection(set(val_words))) == 0)\n",
    "\n",
    "    print(f'Sizes: train = {len(train)}, test = {len(test)}, validation = {len(val)}')\n",
    "    return train, test, val\n",
    "    \n",
    "\n",
    "data_dir = '../diagnostic_classifiers/data/vpc_classification'\n",
    "if not os.path.exists(data_dir):\n",
    "    os.mkdir(data_dir)\n",
    "    \n",
    "train, test, val = split_lexically(dataset)\n",
    "\n",
    "for s, filename in zip([train, test, val], ['train', 'test', 'val']):\n",
    "    with codecs.open(os.path.join(data_dir, 'ids_{}.jsonl'.format(filename)), 'w', 'utf-8') as f_out:\n",
    "        for bnc_id, label, span_text in s:\n",
    "            example = {'bnc_id': bnc_id, 'span_text': span_text.replace('_', ' '), 'label': label}\n",
    "            f_out.write(json.dumps(example) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check: majority baseline is not too strong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_majority_label_per_word(train_set, word_index=0):\n",
    "    \"\"\"\n",
    "    Compute the majority label by word\n",
    "    :word_index: 0 for verb, 1 for preposition\n",
    "    \"\"\"\n",
    "    per_word_labels = defaultdict(list)\n",
    "    for _, label, span_text in train_set:\n",
    "        w = span_text.split('_')[word_index]\n",
    "        per_word_labels[w].append(label)\n",
    "        \n",
    "    per_word_majority_label = {w: Counter(curr_labels).most_common(1)[0][0] \n",
    "                               for w, curr_labels in per_word_labels.items()}\n",
    "    return per_word_majority_label   \n",
    "\n",
    "\n",
    "test_labels = [label for _, label, _ in test]\n",
    "overall_majority_label = Counter([label for _, label, _ in train]).most_common(1)[0][0]\n",
    "test_predictions_all = [overall_majority_label] * len(test)\n",
    "print('Majority overall: {:.2f}%'.format(\n",
    "    100.0 * accuracy_score(test_labels, test_predictions_all)))\n",
    "\n",
    "per_verb_majority_label = get_majority_label_per_word(train, word_index=0)\n",
    "per_prep_majority_label = get_majority_label_per_word(train, word_index=1)\n",
    "\n",
    "test_verbs = [span_text.split('_')[0] for _, _, span_text in test]\n",
    "test_preps = [span_text.split('_')[1] for _, _, span_text in test]\n",
    "\n",
    "test_predictions_verb = [per_verb_majority_label.get(v, overall_majority_label) \n",
    "                         for v in test_verbs]\n",
    "print('Majority by verb: {:.2f}%'.format(\n",
    "    100.0 * accuracy_score(test_labels, test_predictions_verb)))\n",
    "\n",
    "test_predictions_prep = [per_prep_majority_label.get(p, overall_majority_label) \n",
    "                         for p in test_preps]\n",
    "print('Majority by preposition: {:.2f}%'.format(\n",
    "    100.0 * accuracy_score(test_labels, test_predictions_prep)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the dataset is given with the sentence IDs and without the sentences themselves, to comply with the BNC corpus license. To get the sentences, follow the instructions in the repository README file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We re-annotated the dataset to compute human performance. \n",
    "We assume the annotation results are found under `preprocessing/annotation/vpc/batch_results`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_batch_results(result_file, remove_bad_workers=False):\n",
    "    \"\"\"\n",
    "    Load the batch results from the CSV\n",
    "    :param result_file: the batch results CSV file from MTurk\n",
    "    :return: the workers and the answers\n",
    "    \"\"\"\n",
    "    answer_by_worker, answer_by_hit = defaultdict(dict), defaultdict(dict)\n",
    "    workers = set()\n",
    "    incorrect = set()\n",
    "    span_texts = {}\n",
    "    workers_wrong_answers = defaultdict(int)\n",
    "    sent_id_orig_label = {}\n",
    "    \n",
    "    with codecs.open(result_file, 'r', 'utf-8') as f_in:\n",
    "        reader = csv.DictReader(f_in)\n",
    "        for row in reader:\n",
    "            hit_id = row['HITId']\n",
    "            worker_id = row['WorkerId']\n",
    "\n",
    "            # Input fields\n",
    "            sent = row['Input.sent']\n",
    "            orig_label = row['Input.original_label']\n",
    "            \n",
    "            sent_id_orig_label[sent_id] = orig_label\n",
    "            v, p = row['Input.w_first'], row['Input.w_last']\n",
    "            sent_id = row['Input.sent_id']\n",
    "            \n",
    "            # Answer fields\n",
    "            answer = row['Answer.answer.vpc'].lower()\n",
    "            \n",
    "            if row['Answer.answer.incorrect'].lower() == 'true':\n",
    "                incorrect.add(sent_id)\n",
    "                continue\n",
    "\n",
    "            if orig_label != answer:\n",
    "                workers_wrong_answers[worker_id] += 1\n",
    "                \n",
    "            span_texts[sent_id] = ' '.join((v, p))\n",
    "            workers.add(worker_id)\n",
    "            answer_by_worker[worker_id][sent_id] = answer\n",
    "            answer_by_hit[sent_id][worker_id] = answer\n",
    "            \n",
    "    # Remove HITs that were annotated as incorrect by at least one worker\n",
    "    answer_by_hit = {sent_id: answers_by_sent_id \n",
    "                     for sent_id, answers_by_sent_id in answer_by_hit.items()\n",
    "                     if sent_id not in incorrect}\n",
    "    \n",
    "    new_answer_by_worker = {}\n",
    "    for worker_id, curr_answers in answer_by_worker.items():\n",
    "        new_answer_by_worker[worker_id] = {sent_id: answer \n",
    "                                           for sent_id, answer in curr_answers.items()\n",
    "                                           if sent_id not in incorrect}\n",
    "        \n",
    "    answer_by_worker = new_answer_by_worker\n",
    "    num_answers = sum([len(answers_by_worker_id) \n",
    "                       for answers_by_worker_id in answer_by_worker.values()])\n",
    "    \n",
    "    if remove_bad_workers:\n",
    "        workers_wrong_answers = {worker_id: n * 100.0 / len(answer_by_worker[worker_id])\n",
    "                                 for worker_id, n in workers_wrong_answers.items()}\n",
    "\n",
    "        # Remove bad workers: workers that disagreed with many of the previous annotation \n",
    "        bad_workers = {worker_id \n",
    "                       for worker_id, per in workers_wrong_answers.items() if per > 35}\n",
    "        print(f'Removing {len(bad_workers)} bad workers:\\n{bad_workers}')\n",
    "\n",
    "        answer_by_worker = {worker_id: answers_by_worker_id \n",
    "                            for worker_id, answers_by_worker_id in answer_by_worker.items()\n",
    "                            if worker_id not in bad_workers}\n",
    "\n",
    "        for sent_id in answer_by_hit.keys():\n",
    "            answers_by_sent_id = answer_by_hit[sent_id]\n",
    "            answer_by_hit[sent_id] = {worker_id: answer \n",
    "                                      for worker_id, answer in answers_by_sent_id.items()\n",
    "                                      if worker_id not in bad_workers}\n",
    "\n",
    "        num_answers_after_filtering = sum([len(answers_by_worker_id) \n",
    "                                           for answers_by_worker_id in answer_by_worker.values()])\n",
    "        print('Final: {} answers, removed {}.'.format(\n",
    "            num_answers_after_filtering, \n",
    "            num_answers - num_answers_after_filtering))\n",
    "    \n",
    "    return workers, answer_by_worker, answer_by_hit, incorrect, span_texts, sent_id_orig_label\n",
    "\n",
    "\n",
    "results_file = 'vpc/batch_results.csv'\n",
    "workers, answer_by_worker, answer_by_hit, incorrect, span_texts, sent_id_orig_label = load_batch_results(\n",
    "    results_file, remove_bad_workers=True)\n",
    "print(f'Loaded results from {results_file}')\n",
    "print(f'Removed {len(incorrect)}/{len(dataset)} incorrect instances.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computes Fleiss Kappa and percent of agreement between the workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_by_sent_id = {sent: sent_id for sent_id, sent in sent_to_sent_id.items()}\n",
    "\n",
    "\n",
    "def compute_agreement(answer_by_hit, sent_ids):\n",
    "    \"\"\"\n",
    "    Compute workers' agreement (Fleiss Kappa and percent) \n",
    "    \"\"\"\n",
    "    data = []\n",
    "    percent = 0\n",
    "    \n",
    "    for sent_id, worker_answers in answer_by_hit.items():\n",
    "        if sent_id in sent_ids:\n",
    "            curr = [0, 0]\n",
    "\n",
    "            for answer in worker_answers.values():\n",
    "                label = 1 if answer == 'true' else 0\n",
    "                curr[label] += 1\n",
    "\n",
    "            if sum(curr) == 3:\n",
    "                data.append(curr)\n",
    "                curr_agreement = sum([max(0, a-1) for a in curr])        \n",
    "                percent += curr_agreement\n",
    "\n",
    "    kappa = fleiss_kappa(data)\n",
    "    percent = percent * 100.0 / (len(data) * 2)\n",
    "    return kappa, percent\n",
    "\n",
    "\n",
    "kappa, percent = compute_agreement(answer_by_hit, \n",
    "                                   set([sent_id for (sent_id, label, span_text) in test]))\n",
    "print('Fleiss Kappa={:.3f}, Percent={:.2f}%'.format(kappa, percent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the workers majority which we will use to estimate human performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_majority(results):\n",
    "    \"\"\"\n",
    "    Compute the majority label from the worker answers    \n",
    "    :param results: sentence ID to worker answers dictionary\n",
    "    \"\"\"\n",
    "    distribution = { sent_id : Counter(sent_results.values())\n",
    "                    for sent_id, sent_results in results.items() }\n",
    "    \n",
    "    dataset = [{'sent_id': sent_id, \n",
    "                'span_text': span_texts[sent_id],\n",
    "                'label': dist.most_common(1)[0][0]}\n",
    "               for sent_id, dist in distribution.items()\n",
    "               if len(dist) > 0 and dist.most_common(1)[0][1] >= 2]\n",
    "   \n",
    "    return dataset\n",
    "\n",
    "human_annotations = compute_majority(answer_by_hit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the human performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_by_sent_id = {e['sent_id']: e['label'] for e in human_annotations}\n",
    "\n",
    "test_annotations = [(sent_id, label) \n",
    "                    for (sent_id, label, span_text) in test\n",
    "                    if sent_id in gold_by_sent_id]\n",
    "\n",
    "human_accuracy = sum([1 if label == gold_by_sent_id[sent_id] else 0 \n",
    "                      for (sent_id, label) in test_annotations\n",
    "                     ]) * 100.0 / len(test_annotations)\n",
    "\n",
    "print('Number of examples: {}, accuracy: {:.3f}'.format(len(test_annotations), human_accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (lexcomp)",
   "language": "python",
   "name": "lexcomp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
